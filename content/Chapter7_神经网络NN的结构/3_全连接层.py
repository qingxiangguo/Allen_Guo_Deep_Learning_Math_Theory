# _*_ coding=utf-8 _*_

"""
全连接层（也称为线性层或者稠密层）是神经网络中的一种基本层类型。全连接层的每一个神经元都与前一层的所有神经元相连。
这些连接具有权重，全连接层的作用是对输入数据执行线性变换。
"""

# 假设有这样一个神经网络
import torch

import torch.nn as nn

class TextSimilarityModel(nn.Module):
    def __init__(self):
        super(TextSimilarityModel, self).__init__()
        self.fc1 = nn.Linear(300, 128)
        self.fc2 = nn.Linear(128, 300)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = TextSimilarityModel()

"""
当你使用self.fc1 = nn.Linear(300, 128) 的时候，其实是连接了输入层的300神经元和隐藏层的128神经元，然后权重矩阵的形状是(300, 128)。
如果输入是向量（300），就会触发矩阵乘法操作中的：一维向量与二维矩阵相乘，
会将(300)变成(1, 300)，然后与(300, 128)相乘，然后变成(1, 128)，再删除掉预置位，变成(128)的输出。

self.fc1 = nn.Linear(128, 300)则是承接了隐藏层与输出层，输入是刚刚输出的(128)，又会触发矩阵乘法操作中的：一维向量与二维矩阵相乘，
会将(128)变成(1, 128)，然后与(128, 300)相乘，然后变成(1, 300)，再删除掉预置位，变成(300)的输出。

这就是一个三层的神经网络，完成了输入层的300神经元，隐藏层128神经元，输出层300神经元

这个神经网络通常被认为是一个三层网络，包括输入层、隐藏层和输出层。在这个例子中，隐藏层和输出层都由全连接层实现。
当我们说一个神经网络有多少层时，我们通常指的是隐藏层的数量，【而不是计算全连接层的数量】。

所以，对于这个TextSimilarityModel，我们可以描述它为一个具有1个隐藏层的三层神经网络。
这里的隐藏层（self.fc1）和输出层（self.fc2）都是全连接层。输入层没有显式地表示为全连接层，而是通过将输入数据传递给隐藏层实现的。

-----------------------------------------------

但是，这里仅仅是针对单个输入向量（形状为 (300,)）进行说明的。现在我们将其扩展到批量输入。

当我们使用批量输入时，输入矩阵的形状为 (batch_size, input_size)，例如 (10, 300)，表示有 10 个样本，每个样本具有 300 个特征。

在这种情况下，矩阵乘法操作会如下：

第一层全连接层（从输入层到隐藏层）：

输入矩阵的形状：(10, 300)
权重矩阵：(300, 128)
输出矩阵：(10, 128) # 也就是10个样本，每个样本都有128个神经元的输出
第二层全连接层（从隐藏层到输出层）：

输入矩阵：(10, 128)
权重矩阵：(128, 300)
输出矩阵：(10, 300) # 最后10个样本，每个样本都有300个神经元输出

在这个例子中，我们一次处理 10 个样本，通过神经网络的两层全连接层。每一层的权重矩阵都保持不变，
但输入矩阵的形状从单个样本 (1, 300) 扩展到批量样本 (10, 300)。这样，矩阵乘法会在批量样本上进行计算，而不是逐个计算。
输出矩阵的形状也相应地从 (128) # 预置维度被收走 和 (300) # 预置维度被收走 变为 (10, 128) 和 (10, 300)。
"""