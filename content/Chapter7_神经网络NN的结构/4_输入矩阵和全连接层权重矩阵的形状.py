# _*_ coding=utf-8 _*_

"""
在使用 PyTorch 训练神经网络时，输入矩阵和全连接层权重矩阵的形状需要遵循以下规律：

输入矩阵形状：输入矩阵的形状通常为 (batch_size, input_features)，其中 batch_size 是一次处理的数据样本数量，input_features
是每个数据样本的特征数量。

全连接层权重矩阵形状：对于全连接层，权重矩阵的形状为 (input_features, output_features)，其中 input_features 是输入特征数量，output_features 是输出特征数量。偏置向量的形状为 (output_features)。

这里有一个简单的例子来说明这些规律。假设我们有一个三层全连接神经网络，输入层有 4 个特征，隐藏层有 6 个神经元，输出层有 2 个神经元。
我们使用大小为 8 的批量进行训练。在这种情况下，输入矩阵和权重矩阵的形状如下：

输入矩阵形状：(8, 4)
第一个全连接层权重矩阵形状：(4, 6)
第二个全连接层权重矩阵形状：(6, 2)
请注意，每个全连接层的输入特征数量与上一层的输出特征数量相同。在计算过程中，输入矩阵与权重矩阵相乘，得到形状为 (batch_size, output_features)
的输出矩阵。这样，神经网络的输出形状为 (8, 2)，表示每个样本的输出有 2 个特征。

------------

另外一个例子

另外一个神经网络呢，有四层，输入是10个样本，每个样本有300个特征，也就是输入层有300个神经元，第一个隐藏层有128个神经元，
第二个隐藏层有25个神经元，输出层有10个神经元。输入矩阵为(10, 300)，第一个权重矩阵(300, 128), 矩阵乘法做完后，输出(10, 128)形状，然后 第二个权重矩阵为(128,25)，输出形状(10, 25)，最后输出层权重矩阵为(25,10)，输出形状为(10, 10)，就是10个样本，每个样本有10个输出。

输入矩阵形状：(10, 300)
第一个全连接层权重矩阵形状【隐藏层】：(300, 128)
第二个全连接层权重矩阵形状【隐藏层】：(128, 25)
第三个全连接层【输出层】权重矩阵形状：(25, 10)
神经网络的输出形状为 (10, 10)，表示每个样本的输出有 10 个特征。在实际应用中，这种输出形状通常用于多分类任务，
其中 10 个输出特征对应于 10 个不同的类别。最后，通常会在输出层使用 softmax 激活函数，将输出值转换为概率分布，从而确定每个样本的类别。

"""