# _*_ coding=utf-8 _*_

"""
对于二分类问题，最后一层通常使用 sigmoid 激活函数。这是因为 sigmoid 函数将网络的输出值映射到 0 到 1 之间，表示某个类别的概率。
对于二分类问题，我们只需要知道一个类别的概率，另一个类别的概率就是 1 减去这个概率。

尽管 softmax 函数也可以用于二分类问题，但它更适合用于多分类问题，因为它将网络输出值映射到一个概率分布，使得所有类别的概率之和为 1。
在二分类问题中，使用 softmax 函数会增加计算复杂性，而没有额外的优势，因此 sigmoid 函数是更常用的选择。

假设我们要构建一个神经网络来进行猫和狗的二分类任务。在这个任务中，我们需要知道输入图像是猫的概率，从而可以推断出是狗的概率。
这里我们将猫视为正类（标签为 1），狗视为负类（标签为 0）。

使用 Sigmoid 激活函数：

在这种情况下，我们的神经网络最后一层只有一个输出节点。对于输入图像，我们将网络输出值传递给 sigmoid 函数，得到一个介于 0 和 1 之间的概率值 p。
这个概率值表示输入图像是猫的概率。

例如，如果 p = 0.8，那么输入图像是猫的概率为 80%，是狗的概率为 1 - p = 0.2，即 20%。

使用 Softmax 激活函数：

在这种情况下，我们的神经网络最后一层需要有两个输出节点，分别对应猫和狗。对于输入图像，我们将网络输出值传递给 softmax 函数，
得到两个概率值 p1 和 p2，它们分别表示输入图像是猫和狗的概率。

例如，如果 p1 = 0.8，p2 = 0.2，那么输入图像是猫的概率为 80%，是狗的概率为 20%。

从这个例子中可以看出，对于二分类问题，使用 sigmoid 函数更简单，因为我们只需要一个输出节点，而 softmax 函数需要两个输出节点。
同时，在二分类问题中，sigmoid 函数可以直接给出一个类别的概率，而另一个类别的概率可以通过 1 减去这个概率得到，因此没有必要使用 softmax 函数。

在二分类问题中，我们通常使用二元交叉熵损失函数（Binary Cross-Entropy Loss，简称 BCE）与 sigmoid 激活函数配合使用。
BCE 损失函数可以很好地处理二分类任务中的概率输出。

二元交叉熵损失函数的定义如下：

L(y, p) = -[y * log(p) + (1 - y) * log(1 - p)]

其中，y 是真实标签（0 或 1），p 是预测概率（经过 sigmoid 激活函数处理后的输出值）。

当 y = 1（正类）时，损失函数为 L(y, p) = -log(p)。此时，预测概率 p 接近 1 时，损失值接近 0；预测概率 p 接近 0 时，损失值会变得非常大。

当 y = 0（负类）时，损失函数为 L(y, p) = -log(1 - p)。此时，预测概率 p 接近 0 时，损失值接近 0；预测概率 p 接近 1 时，损失值会变得非常大。

通过这种方式，二元交叉熵损失函数鼓励模型产生正确类别的高概率预测，同时惩罚错误类别的高概率预测。这使得它与 sigmoid 激活函数在二分类问题中非常有效。
"""