# _*_ coding=utf-8 _*_

"""
当我们使用 sigmoid 激活函数时，神经网络在反向传播过程中可能会出现梯度消失的问题，导致参数无法更新。这是因为在 sigmoid 函数的两端，
即函数趋近 0 和 1 的时候，导数趋近于 0，反向传播中的梯度也会趋近于 0，因此在这些神经元处的梯度会变得非常小，从而导致梯度消失。

为了解决梯度消失问题，可以使用一些其他的激活函数，比如 ReLU（Rectified Linear Unit）函数。ReLU 函数在输入大于 0 时的导数为常数 1，
因此避免了梯度消失的问题。同时，ReLU 函数的计算速度也很快，计算成本较低，因此在深度学习中得到了广泛应用。

除了 ReLU 函数之外，还有其他的一些激活函数，比如 Leaky ReLU、ELU 等，它们也能够在一定程度上避免梯度消失的问题。

下面是一个简单的例子来说明这个问题：假设我们有一个深度神经网络，它有很多层，每一层都使用 sigmoid 激活函数。在反向传播过程中，
如果我们发现某一层神经元处的梯度接近于 0，那么这些神经元的权重就无法更新，而这些权重所代表的特征对模型的性能影响较大，这就可能导致模型的性能下降。
因此，为了解决这个问题，我们可以尝试使用其他的激活函数，比如 ReLU，来代替 sigmoid 函数。

如果是只有几个sigmoid层的浅层神经网络,这并不会引起很大的问题。然而，当非常多的sigmoid层时，它会导致梯度太小而无法训练，这就是我们说的梯度消失。
神经网络的梯度通过反向传播来得到，简单的说，反向传播通过从最终层到初始层，误差逐层传播来得到梯度，通过链式求导法则，
每一层的导数会乘到一起来计算初始层的导数。
然后，当n个隐层使用像sigmoid这样的激活函数时，会有n个非常小的导数值乘到一起，这样，随着我们逐层反向传播到初始层，梯度会呈指数级的下降。
显然极小的梯度值使得初始层的权值和偏置几乎无法有效的更新。而这些初始层，在识别输入数据的基础模式是至关重要的，它的不准确将导致整个神经网络的不准确。

梯度消失可能会导致神经网络的训练停滞不前，很难收敛。当反向传播算法无法更新前几层的权重时，这些层将无法学习到有关输入数据的重要特征，
导致网络性能不佳。此外，由于梯度消失，网络可能需要更长的时间才能学习到特定的模式，
从而导致训练时间更长，甚至在某些情况下可能无法训练成功。因此，解决梯度消失问题对于训练深层神经网络非常重要。

【下面通过一个实例来理解梯度消失】
假设我们有一个神经网络，只有输入层x,输出层y，激活函数为sigmoid，权重为w，偏置项为b
那么有y预测 = wx +b

假设我们的目标是使损失函数最小化，即：

L = (y真实 - y预测)^2

其中，y真实是真实的类别标签。我们可以通过反向传播算法来更新权重w，以使损失函数最小化。

如果更新权重w, 需要求dL/dw

而dL/dw = dL/dy预测 * dy预测/dw 【根据链式法则】

L = (y真实 - y预测)^2

y预测 = sigmoid(w*x + b)

所以 dL/dy预测 = -2(y真实 - y预测)
dy预测/dw = sigmoid(w*x + b) * (1 - sigmoid(w*x + b)) *x

所以dL/dw = dL/dy预测 * dy预测/dw = -2(y真实 - y预测) * sigmoid(w*x + b) * (1 - sigmoid(w*x + b)) *x

又由于x很大，所以w*x + b也很大，因此sigmoid(w*x + b)的导数几乎为0

所以上面式子中，sigmoid(w*x + b) * (1 - sigmoid(w*x + b)) 的部分为0

因此导致dL/dw为0，待优化的权重参数w不会更新
--------------------------------------
【梯度消失解决办法】最简单的解决方法是使用另一种激活函数，比如 ReLU，它通常不会产生一个很小的导数
另外还有一个方法，是batch normalization（批归一化），后面再详解，这里先不表


"""