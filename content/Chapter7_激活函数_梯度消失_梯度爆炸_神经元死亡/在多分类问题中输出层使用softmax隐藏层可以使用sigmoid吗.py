# _*_ coding=utf-8 _*_

"""
对于多分类问题，通常使用softmax函数作为输出层的激活函数。
Softmax函数可以将任意实数向量映射到概率向量，例如预测图像的类别。这一点比较好理解。

但是在多分类问题中输出层使用softmax激活函数，那么隐藏层用什么激活函数呢？是sigmoid吗？

隐藏层的激活函数可以是任意合适的激活函数，不一定是sigmoid函数。

【下面这些函数都是常用于隐藏层的】
常见的隐藏层激活函数有ReLU、Tanh、Sigmoid、Leaky ReLU、Exponential Linear Unit等。
选择具体的激活函数要根据具体的问题和数据进行评估。

ReLU函数是一种简单且高效的激活函数，它在神经网络中广泛使用。它的定义为：f(x) = max(0, x)，当x大于0时，输出x；当x小于等于0时，输出0。
ReLU函数通常在隐藏层使用，它可以有效地避免梯度消失的问题（在sigmoid和tanh函数中存在的问题），并且比较容易计算。

接下来是Tanh函数，它的定义为：f(x) = 2/(1 + exp(-2x)) - 1，
可以将其理解为将输入的实数值映射到-1到1之间的值。因此，它通常在数据有较强的线性关系时使用，
特别是在时序问题中（例如预测股票价格）。

再提到Sigmoid函数，它的定义为：f(x) = 1 / (1 + exp(-x))，可以将其理解为将输入的实数值映射到0到1之间的概率值。
它通常用于二分类问题，因为它的输出值可以表示为给定输入的概率。

Leaky ReLU函数是一种对ReLU函数的改进，它的定义为：f(x) = max(αx, x)，其中α是一个小的正数，通常取0.01。
Leaky ReLU函数的目的是解决ReLU函数存在的“死神经元”问题，当x < 0时，Leaky ReLU函数的输出不是0，而是αx。
因此，即使在输入为负数时，
该神经元仍然有一定的输出。这样，即使某些神经元被长期不激活，它们仍然可以对网络的训练做出一定的贡献。

Exponential Linear Unit (ELU)是另一种改进的激活函数，它的定义为：

f(x) =
α(exp(x) - 1) if x < 0
x if x >= 0

其中α是一个超参数，通常取值为1。ELU函数的设计目的是改善ReLU和Leaky ReLU的性能。与ReLU不同，ELU函数具有更强的非线性性，
并且在x < 0时具有负数的输出，这有助于防止神经元“死亡”。与Leaky ReLU不同，ELU函数具有更平缓的输出转折点，
对于某些数据集更有利。
"""

