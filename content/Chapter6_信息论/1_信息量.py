# _*_ coding=utf-8 _*_

"""
先上信息量的公式，等一下讲怎么推导的：

信息量和概率的关系可以通过香农提出的信息熵（Shannon entropy）公式得到。假设某个事件 x发生的概率为 p(x)，
则 x 发生的信息量可以表示为：

I(x)=-log2(p(x))，也就是概率以2为底，取log，然后再取负

---------------信息量的含义-----------------

这个公式的物理意义是，当某个事件发生的概率 p(x) 很小时，
它所提供的信息量 $I(x)$ 就越大。因为此时事件如果一旦发生，是比较罕见的，它为我们提供了更多的信息。

如果中国足球队与巴西足球队进行比赛，中国队胜利的概率是 1/20，那么胜利的信息量可以用以下公式计算：

-log2(1/20) = 4.32 bits (比特)，因为这个事件的概率比较低，所以我们需要用比较多的 bits 来表示它发生的信息量。

中国队与巴西队比赛，中国队竟然赢了，这是一个信息量很大的事情
"""