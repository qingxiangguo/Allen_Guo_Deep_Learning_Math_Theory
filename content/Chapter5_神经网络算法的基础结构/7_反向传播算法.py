# https://baijiahao.baidu.com/s?id=1736517058251213795&wfr=spider&for=pc

# _*_ coding=utf-8 _*_

"""
反向传播则用于计算损失函数的梯度并更新参数，利用了链式传播法则，以一个例子来理解

比如【7_批量梯度下降_Batch_Gradient_Descent】中的例子

假设我有三个点是(4,3), (1,3), (3, 7), 这三个点是训练用的三个样本。
用线y=3x+b拟合这三个点， 使用BGD梯度下降, 学习率=0.01, b初始值为0, 目的是找最合适的b点

【第一轮迭代】
真实值 y1 = 3, 预测值 y^1 = 3*x1 + b = 3(4) + 0 = 12

损失函数L1 = (y真实值 - y预测值)^2 = (3 - 12)^2 = 81

真实值 y2 = 3, 预测值 y^2 = 3*x2 + b = 3(1) + 0 = 3

损失函数L2 = (y真实值 - y预测值)^2 = (3 - 3)^2 = 0

真实值 y3 = 7, 预测值 y^3 = 3*x3 + b = 3(3) + 0 = 9

损失函数L3 = (y真实值 - y预测值)^2 = (7 - 9)^2 = 4

梯度Δ1 = -2(y真实值 - y预测值) = 18  （指的是L对b的梯度，也就是L对你要优化的参数的梯度）
梯度Δ2 = -2(y真实值 - y预测值) = 0
梯度Δ3 = -2(y真实值 - y预测值) = 4

平均梯度 = (18 + 0 + 4)/3 （所有梯度的平均值）
-------------------------------------------------------

这其中，梯度Δ1 = -2(y真实值 - y预测值) = 18  （指的是L对b的梯度，也就是L对你要优化的参数的梯度）
是怎么来的呢？其实就是利用了链式法则，dy/dz = dy/du * du/dz

假设损失函数：L = (y真实 - y预测)^2    y预测 = 3x + b (目标函数)
我们现在要求L对于b的偏导，也就是梯度， 由于链式法则可知，dL/db = dL/d(y预测) * d(y预测) * db
翻译过来就是，损失函数对于待优化参数b的偏导 = 损失函数对于目标函数的偏导 * 目标函数对待优化参数b的偏导
要注意，y真实只是一个常数，y预测才是你根据目标函数计算出来的

损失函数对于待优化参数b的偏导 = 损失函数对于目标函数的偏导 * 目标函数对待优化参数b的偏导
损失函数对于目标函数的偏导 dL/dy预测 = (y真实 - y预测)^2 对y预测求偏导， 由于是复合函数
结果 = 2*（-y预测+y真实） * -1 = 2*（y预测 - y真实）

同理，目标函数对待优化参数b的偏导  y预测 = 3x + b，所以偏导 = 1

所以才有梯度Δ1 = -2(y真实值 - y预测值) = 18  （指的是L对b的梯度，也就是L对你要优化的参数的梯度）
"""