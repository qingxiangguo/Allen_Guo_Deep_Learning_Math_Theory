# _*_ coding=utf-8 _*_

"""

在一些入门教程中，为了简化问题和便于理解，可能会忽略多路径的情况。

在上面举的这个神经网络例子中，可以观察到链式法则的多途径性，也就是损失函数对同一个参数的表示，
可以有多种不同的链式法则，比如损失函数 L 对 w21 的偏导数

∂L/∂w21 = ∂L/∂h8 * ∂h8/∂h5 * ∂h5/∂h2 * ∂h2/∂w21

也可以表示为

∂L/∂w21 = ∂L/∂h8 * ∂h8/∂h6 * ∂h6/∂h2 * ∂h2/∂w21

∂L/∂w21 = ∂L/∂h8 * ∂h8/∂h7 * ∂h7/∂h2 * ∂h2/∂w21

那是不是可以理解为，在实际操作中，两条路径都可以用来计算损失函数L对w21的偏导数呢？其实并不是的！！

实际上，我们需要将这三条路径的偏导数相加以获得完整的梯度信息。

总的梯度计算为：
∂L/∂w21 = (∂L/∂h8 * ∂h8/∂h5 * ∂h5/∂h2 * ∂h2/∂w21) + (∂L/∂h8 * ∂h8/∂h6 * ∂h6/∂h2 * ∂h2/∂w21) +
∂L/∂h8 * ∂h8/∂h7 * ∂h7/∂h2 * ∂h2/∂w21

这样，我们就可以得到损失函数L对w21的正确偏导数。在实际操作中，我们通常会在编程实现时自动计算所有有效路径的梯度，并将它们相加。

在使用反向传播更新参数w21时，我们需要求损失函数L对w21的偏导数。这个偏导数包括了所有有效路径对该参数的梯度贡献。

这个总梯度就是我们在更新参数w21时使用的梯度。

梯度相加的依据也是链式法则公式 dz/dt = ∂z/∂u * du/dt + ∂z/∂v * dv/dt

在神经网络中，损失函数L对参数的偏导数需要考虑所有可能的有效路径。对于每个路径，我们都可以应用链式法则来计算偏导数。
当有多个有效路径时，需要将各个路径上的偏导数相加以得到最终的梯度。这是因为在微积分中，当一个函数受多个因素影响时，这些因素对函数的偏导数是可加的。

在神经网络优化中，将各个路径上的梯度相加可以确保计算出的梯度更加准确，从而更好地优化网络参数。这也是为什么在实际应用中，特别是在处理复杂网络时，需要考虑所有有效路径以获得更好的优化效果。

我们需要考虑t通过u和v两条路径影响z的情况。根据链式法则，我们需要计算t对z的偏导数，这需要将两条路径上的偏导数相加。具体来说，就是计算∂z/∂u * du/dt 和 ∂z/∂v * dv/dt，然后将它们相加。

这个公式说明了当一个变量（如t）通过多个途径（如u和v）影响另一个变量（如z）时，我们需要将所有路径上的梯度（即偏导数）相加以计算最终的梯度。这个原理同样适用于神经网络中的梯度计算，当有多个有效路径时，需要将各个路径上的梯度相加以得到最终的梯度。这可以确保我们得到准确的梯度，从而更好地优化网络参数。
"""