# _*_ coding=utf-8 _*_

"""
这个例子是一个比较全面的例子，可以理解反向传播，梯度下降，激活函数，链式法则
这个神经网络包含三层：输入层、隐藏层和输出层。
第一层是输入层，包含两个神经元i1，i2，和截距项b1；第二层是隐含层，包含两个神经元h1,h2和截距项b2，第三层是输出o1,o2，
每条线上标的wi是层与层之间连接的权重，激活函数我们默认为sigmoid函数。

i1───w1────> h1───w5────>o1 ----> Eo1 损失函数
│  ╲   w2 ╱   │ ╲   w6  ╱                   ╲
     ╲  ╱        ╲     ╱                      E(total) 总损失函数 = Eo1 + Eo2
│   ╱     ╲ w3│  ╱      ╲ w7                ╱
i2───w4────> h2────w8────>o2 ----> Eo2 损失函数

现在对参数赋上初值, w1 = 0.15 , w2 = 0.2 , w3 = 0.25, w4 =0.3 , w5 = 0.4, w6 = 0.45, w7 = 0.5, w8 = 0.55
截距b1 = 0.35, b2 = 0.6

目标：给出输入数据i1,i2 (0.05和0.10)，使输出尽可能与原始输出o1,o2 (0.01和0.99)接近

第一步：正向传播

输入层，到隐藏层

net(h1) = w1 * i1 + w2 * i2 + b1 = 0.15 * 0.05 +0.2 * 0.1 +0.35 * 1 = 0.3775
# net的意思是净输出，就是还没有经过激活函数处理的值

神经元h1的输出o1:(此处用到激活函数为sigmoid函数)：

out(h1) = sigmoid(net(h1)) = 0.593269992 # 省略计算步骤

同理，神经元h2的输出o2:

out(h2) = sigmoid(net(h2)) = 0.596884378 # 省略计算步骤

然后隐藏层到输出层：

net(o1) = w5 * out(h1) +w6 * out(h2) + b2

net(o1) =  0.4 * 0.593269992 +0.45 * 0.596884378 +0.6 = 1.105905967

out(o1) = sigmoid(net(o1)) = 0.75136507 # 省略计算步骤

同理，out(o2) = sigmoid(net(o2)) = 0.772928465

这样正向传播的过程就结束了。我们得到输出值为[0.75136079 , 0.772928465]，与实际值[0.01 , 0.99]相差还很远，
现在我们对误差进行反向传播，更新权值，重新计算输出。

第二步：反向传播

首先明确，我们需要优化的总损失函数E(total) = EO1 + EO2，也就是两个输出各自的误差之和

E(total) = 1/2((真实值1 - out(o1))^2 + 1/2((真实值2 - out(o2))^2

代入可得，E(total) = 0.298371109

下面先更新参数w5(连接着h1和o1)，需要求E(total)对w5的梯度，也就是E(total)对w5求偏导

由于w5对E(total)的影响路径有且只有一条，也就是: w5 ────>o1 ────>Eo1 ────>E(total)

所以链式法则在这一条路径上起作用就行，即：

∂E(total)/∂w5 = ∂E(total)/∂Eo1 * ∂Eo1/∂out(o1) * ∂out(o1)/∂net(o1) * ∂net(o1)/∂w5

而上面的子项都是可以求出来的

由于 E(total) = Eo1 + Eo2，所以∂E(total)/∂Eo1  = 1

由于 Eo1 = 1/2((真实值1 - out(o1))^2，所以∂Eo1/∂out(o1) = 2 * 1/2 * ((真实值1 - out(o1)) * -1 = 0.74136507

由于out(o1) = sigmoid(net(o1))，所以∂out(o1)/∂net(o1) = sigmoid(net(o1)) * (1 - sigmoid(net(o1))) = 0.186815602 # sigmoid函数求导公式

由于net(o1) = w5 * out(h1) +w6 * out(h2) + b2，所以∂net(o1)/∂w5 = out(h1) = 0.593269992

最后三者相乘，∂E(total)/∂w5 = = 1 * 0.74136507 * 0.186815602 * 0.593269992 = 0.08216704

假设学习率为0.5，我们来更新w5的值，w5+ = w5 - 0.5 * 0.08216704 = 0.35891648

同理，可以更新w6+ = 0.408666186, w7+ = 0.511301270, w8+ = 0.561370121

下面更新隐藏层权重，w1，这个情况更为复杂一点

因为w1对E(total)的影响路径有两条，也就是:

w1 ────> h1 ────> o1 ────> Eo1 ────> E(total)

w1 ────> h1 ────> o2 ────> Eo2 ────> E(total)

而链式法则有一个表述形式是：dz/dt = ∂z/∂u * du/dt + ∂z/∂v * dv/dt

也就是要求dz对dt的偏导，需要求每一条路径的和，其中，每一条路径都是层层分子分母颠倒相乘

所以，∂E(total)/∂w1是一个和的形式，即：

∂E(total)/∂w5 = ∂E(total)/∂Eo1 * ∂Eo1/∂out(o1) * ∂out(o1)/∂net(o1) * ∂net(o1)/∂out(h1) * ∂out(h1)/∂net(h1) * ∂net(h1)/∂w1

+ ∂E(total)/∂Eo2 * ∂Eo2/∂out(o2) * ∂out(o2)/∂net(o2) * ∂net(o2)/∂out(h1) * ∂out(h1)/∂net(h1) * ∂net(h1)/∂w1

= 0.00043856

更新w1的值，w1+ = w1 - 0.5 * 0.00043856 = 0.149780716

同理，可以更新w2+ 0.19956143 = , w3+ = 0.24975114, w4+ = 0.299550229

这样误差反向传播法就完成了，最后我们再把更新的权值重新计算，不停地迭代，在这个例子中第一次迭代之后，总误差E(total)
由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为[0.015912196,0.984065734](原输入为[0.01,0.99]),
证明效果还是不错的。

"""