# https://baijiahao.baidu.com/s?id=1736517058251213795&wfr=spider&for=pc

# _*_ coding=utf-8 _*_

"""
反向传播则用于计算损失函数的梯度并更新参数，利用了链式传播法则，以一个例子来理解

在下面这个简单的例子中，只有一个路径连接损失函数和待优化参数 b，因此没有涉及到多个梯度相加的问题。
这个例子展示了如何应用链式法则计算梯度，并用于优化参数的基本思路。

然而，在更复杂的神经网络中，可能会有多个路径连接损失函数和待优化参数，这时我们需要考虑所有路径的贡献。
在这种情况下，我们需要沿着每个路径分别计算梯度，并将所有路径的梯度相加以获得损失函数对待优化参数的总梯度。
这样可以确保我们在优化过程中考虑了参数在所有路径上的影响，从而获得更准确的梯度信息。

比如【7_批量梯度下降_Batch_Gradient_Descent】中的例子

假设我有三个点是(4,3), (1,3), (3, 7), 这三个点是训练用的三个样本。
用线y=3x+b拟合这三个点， 使用BGD梯度下降, 学习率=0.01, b初始值为0, 目的是找最合适的b点

【第一轮迭代】
真实值 y1 = 3, 预测值 y^1 = 3*x1 + b = 3(4) + 0 = 12

损失函数L1 = (y真实值 - y预测值)^2 = (3 - 12)^2 = 81

真实值 y2 = 3, 预测值 y^2 = 3*x2 + b = 3(1) + 0 = 3

损失函数L2 = (y真实值 - y预测值)^2 = (3 - 3)^2 = 0

真实值 y3 = 7, 预测值 y^3 = 3*x3 + b = 3(3) + 0 = 9

损失函数L3 = (y真实值 - y预测值)^2 = (7 - 9)^2 = 4

【这个公式是根据链式法则来的，下面会讲】梯度Δ1 = -2(y真实值 - y预测值) = 18  （指的是L对b的梯度，也就是L对你要优化的参数的梯度）
梯度Δ2 = -2(y真实值 - y预测值) = 0
梯度Δ3 = -2(y真实值 - y预测值) = 4

平均梯度 = (18 + 0 + 4)/3 （所有梯度的平均值）
-------------------------------------------------------

这其中，梯度Δ1 = -2(y真实值 - y预测值) = 18  （指的是L对b的梯度，也就是L对你要优化的参数的梯度）
是怎么来的呢？其实就是利用了链式法则，dy/dz = dy/du * du/dz

假设损失函数：L = (y真实 - y预测)^2    y预测 = 3x + b (目标函数)
我们现在要求L对于b的偏导，也就是梯度， 由于链式法则可知，dL/db = dL/d(y预测) * d(y预测)/db
翻译过来就是，损失函数对于待优化参数b的偏导 = 损失函数对于目标函数的偏导 * 目标函数对待优化参数b的偏导
要注意，y真实只是一个常数，y预测才是你根据目标函数计算出来的

损失函数对于待优化参数b的偏导 = 损失函数对于目标函数的偏导 * 目标函数对待优化参数b的偏导
损失函数对于目标函数的偏导 L = (y真实 - y预测)^2

dL/d(y预测) = 2*（-y预测+y真实） * -1 = 2*（y预测 - y真实）

同理，目标函数对待优化参数b的偏导, y预测 = 3x + b，所以偏导 = 1

所以才有梯度Δ1 = -2(y真实值 - y预测值) = 18  （指的是L对b的梯度，也就是L对你要优化的参数的梯度）
"""