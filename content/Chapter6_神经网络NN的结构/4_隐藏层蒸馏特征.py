# _*_ coding=utf-8 _*_

"""
隐藏层的每一个节点都被赋予了特征提取的任务，
它们通过对输入层不同神经单元的兴奋有不同的接收权重（即重视程度）来实现该任务。
比如，假设隐藏层中的第一个神经单元（可以类比为一个工人）对应了图片中的模式A，
这意味着它更加偏向于重視图片中编号为4和7的神经单元（两个点），而对其他神经单元（点）的兴奋的重视程度要低一些
。当隐藏层的神经单元的兴奋值超过了它自身的阈值，它就会引发兴奋，并向输出层传递信息。
同理，隐藏层中其他神经单元也是如此工作的。

这里的兴奋指的就是神经单元被激活，也就是说，它对输入信息的识别达到了一定程度，会传递兴奋信息。

当设计深度学习模型时，隐藏层神经元数量是一个重要的设计决策。通常情况下，所有隐藏层都使用相同数量的神经元就足够了。
但是，在某些情况下，如果第一层更大，后续层更小，可以对模型性能产生更好的影响。因为第一层可以学习较低阶的特征，
然后将这些特征馈入后续层，以提取更高阶的特征。

最后，隐藏层的神经元数量是通过不断试验来确定的。因此，建议从一个较小的数值开始，例如1到5层和1到100个神经元。

低阶特征指的是较基础的、易于学习的特征，如图片中的线条、颜色等。高阶特征指的是通过结合低阶特征而提取出的更抽象的、
更高级的特征，如图片中的物体形状、表情等。

想象一下，如果你是一个人工智能系统，并需要对图片进行分类，那么你在第一层可以学习到图片中的线条、
颜色等低阶特征，在后续层中可以通过结合这些低阶特征，提取出图片中的物体形状、表情等高阶特征，从而达到分类的目的。

总的来说，通过不断结合低阶特征，可以提取出更高阶、更复杂的特征，使得神经网络的分类效果更好。

在深度学习中，神经网络的每一层都可以看作是一个特征过滤器，其中每一层可以学习到不同级别的特征，
并将这些特征递交给后续层，一层层地抽象更高阶的特征。低阶特征通常是指基本特征，
例如图像中的边缘，颜色，形状等。高阶特征则是通过将低阶特征组合而形成的更复杂的特征，
例如图像中的人脸、动物等。如果将图像的处理过程看作从低阶到高阶的过程，
则可以将深度学习的每一层看作是抽象出特征的不同阶段。

这些特征是神经网络通过训练学习到的，而不是人为指定的。神经网络通过使用大量的训练样本数据来学习提取特征的技巧。
训练过程中，网络可以通过反复试错和修正来逐步提高对数据的理解能力，从而学习到对数据有用的特征。
"""