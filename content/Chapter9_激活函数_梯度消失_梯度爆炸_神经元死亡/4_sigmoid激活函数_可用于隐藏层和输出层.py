# _*_ coding=utf-8 _*_

"""
Sigmoid函数是一种常用的激活函数，也叫做逻辑函数，是一种非线性函数。在深度学习中，sigmoid函数通常被用于二分类问题中的输出层，
将输入的连续值压缩到0到1的范围内，可以看做是输出层的“激活函数”。Sigmoid函数的表达式如下：

sigmoid(x) = 1 / (1 + e^(-x))

Sigmoid函数的特点是，当输入接近正无穷时，函数的值趋近于1，当输入接近负无穷时，函数的值趋近于0。同时，Sigmoid函数的导数可以表示为：

sigmoid'(x) = sigmoid(x) * (1 - sigmoid(x))

Sigmoid函数的优点是，它的输出在0到1之间，可以看做是概率值，非常适合用于二分类问题中的输出层，例如将图像识别为猫或狗。

Sigmoid 激活函数存在的不足主要有三个方面。

【梯度消失】首先，Sigmoid 函数在输出趋近 0 和 1 的时候，变化率会变得平坦，也就是说，Sigmoid 的梯度趋近于 0。
这会导致神经网络中的饱和神经元无法进行权重更新，从而导致梯度消失问题，影响神经网络的反向传播算法。

【非零对称】其次，Sigmoid 函数的输出不以零为中心，输出恒大于0。这会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），
并进一步使得梯度下降的收敛速度变慢。

【成本高】最后，Sigmoid 函数计算成本高昂，需要使用 exp() 函数，与其他非线性激活函数相比，计算速度较慢，因此在实际应用中需要考虑到计算效率的问题。

因此，在实际应用中，为了避免上述问题，可以采用其他更加优秀的激活函数，如 ReLU、Leaky ReLU、ELU 等。
ReLU会避免在反向传播时梯度消失。
"""

