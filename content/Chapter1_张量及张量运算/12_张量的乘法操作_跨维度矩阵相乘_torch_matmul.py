# _*_ coding=utf-8 _*_
'''
torch.matmul,两个张量的矩阵乘积。但是它可以利用python中的广播机制，处理一些维度不同的tensor结构进行相乘操作。这也是该函数与torch.mm()和torch.bmm()区别所在。
他的输出取决于输入情况
torch.matmul(input, other) 输入格式，其中input和other的顺序，是有意义的，矩阵乘法运算，不能随便交换
matmul函数没有强制规定维度和大小，可以用利用广播机制进行不同维度的相乘操作
而且矩阵和向量的乘积，以及向量和矩阵的乘积，是不能弄混的
'''
import torch

# 【第一种情况】，若两个张量都是一维的，则返回两个向量的点积运算结果（标量）：
x = torch.tensor([1,2])  # 一维向量
y = torch.tensor([3,4])   # 一维向量
print(torch.matmul(x, y))  # 返回tensor(11)，是个标量
# print(torch.mm(x, y)) 不能运算，因为torch.mm只能处理二维数组的结果，维度高了低了都不行

# 如果试试torch.mul，就是完成对位相乘
print(torch.mul(x, y)) # 输出tensor([3, 8])，虽然是一维张量，但也可以完成对位相乘

# 【第二种情况】，如果两个矩阵是二维的，返回矩阵相乘结果

a = torch.tensor([[1, 2],
                  [3, 4],
                  ])

b = torch.tensor([[5, 6, 7],
                  [8, 9, 10],
                  ])

print(torch.matmul(a, b))
print(torch.mm(a, b))

'''
# 因为是矩阵相乘，所以这里torch.matmul和torch.mm结果一样
tensor([[21, 24, 27],
        [47, 54, 61]])
tensor([[21, 24, 27],
        [47, 54, 61]])
'''

# 【第三种情况：比较难理解的】，如果一个一维向量(input)和一个二维矩阵(other)的乘积运算, 下面的x,y是无法进行正常的对位相乘操作的  vector X matrix
# 这种情况下，若input为一维，other为二维，则先将input的一维向量扩充到二维（维数前面插入长度为1的新维度），(广播机制)
# 然后进行【矩阵乘积】，得到结果后再将此维度去掉，得到的与input的维度相同。
# 说白了，就是matmul函数，对维度的控制比较轻松，可以将前面的一维向量升维后，进行矩阵相乘，最后的结果再降维
# 来看看官方的解释：如果第一个参数是一维的，而第二个参数是二维的，为了进行矩阵乘法，会在其维度上预加一个1。
# 注意，这里的维度上预加一个1，不是广播，相当于加一个中括号，方便进行矩阵乘法，要求这两个维度自动符合矩阵乘法的行列规则
# 不会进一步自动广播，如果不满足，则会报错
# 矩阵乘法后，预置的维度将被删除，也就是会降一维
x = torch.tensor([1, 2])   # 虽然是一维张量，但可以看成一个1行2列的二维张量，shape是(2)

y = torch.tensor([[5,6,7],  # 2行3列，shape是(2,3)
                  [8,9,10]])

print(torch.matmul(x,y))

# 在计算的时候，matmul会将x的shape由(2)变成(1, 2)，相当于x.reshape(1, 2), x = tensor([[1, 2]])，然后再完成二维矩阵相乘
# 获得结果为tensor([[21, 24, 27]])，shape为(1, 3)，然后再把加进去的维度抽掉，相当于变成shape(3)，也就是输出[21, 24, 27]


'''
tensor([21, 24, 27]) # 比如说这里就降维，和张量x的维度是一致的了
'''

# 但是【第三种情况】还有一个前提，就是这个一维向量，升维后，必须满足，矩阵相乘的条件才行，就是A列数，等于B行数
# 否则就如上面提到的，matmul不会替你广播自适应size
# 举个反面例子
x = torch.tensor([1, 2, 3])   # shape = (3)

y = torch.tensor([[5,6,7],  # 2行3列  shape = (2, 3)
                  [8,9,10]])

# print(torch.matmul(x, y))  # 会报错RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x3 and 2x3)
print(x*y)  # 但是对位相乘，倒是可以计算的，因为torch.mul()支持广播

# 【第四种情况】，如果一个二维矩阵(input)和一个一维向量(other)的乘积运算，也就是matrix x vector
# 如果第一个参数是 2 维，第二个参数是 1 维，返回结果为 the matrix-vector product (矩阵向量乘积)
# 注意这里的运行逻辑和第三种情况，vector X matrix中的加维和抽维是完全不一样的，这种情况中完全没有进行矩阵乘积，而是进行了矩阵向量乘积
# 矩阵向量积有自己的运算公式，输出结果为[1*7+2*8+3*9, 4*7+5*8+6*9] = [50, 122]，也就是计算点积
# 该情况下不支持广播，matrix的列数必须要和vector的列数一致才能进行计算
# 矩阵向量乘积的诀窍是，size为(7,8)的和size(8)相乘，得到size = (7)

x = torch.tensor([[1,2,3],  # 2行3列  shape(2, 3)
                  [4,5,6]])

y = torch.tensor([7, 8, 9])  # 一维  shape(3)

print(torch.matmul(x,y))  # tensor([50, 122])

# 【第五种情况】torch.matmul涉及到3维及3维以上的运算，那么又分几种情况：如果第一个元素是一维向量，第二个是三维批量矩阵,也就是vector X matrix
# 这里的逻辑其实就和【第三种情况】一样，相当于为了能够矩阵乘积，将ten1升维为(1, 1, 4)，相当于test = ten1.reshape(1, 1, 4)，然后再进行矩阵乘积计算
# 最后再将结果降维一个，生成一个二维的数组
ten1 = torch.rand(4)
ten2 = torch.rand(2, 4, 3)
print("ten1 =", ten1)
print("ten2 =", ten2)
print(torch.matmul(ten1, ten2).shape) # torch.Size([2, 3])

'''
ten1 = tensor([0.9703, 0.6052, 0.4702, 0.4831])
ten2 = tensor([[[0.5386, 0.1360, 0.0691],
         [0.5295, 0.5324, 0.4595],
         [0.5544, 0.8045, 0.1428],
         [0.7791, 0.2247, 0.0549]],

        [[0.3062, 0.3107, 0.2927],
         [0.8905, 0.8245, 0.2240],
         [0.0562, 0.4877, 0.1918],
         [0.7788, 0.9711, 0.7397]]])
tensor([[1.4801, 0.9410, 0.4388],
        [1.2386, 1.4988, 0.8670]])
'''

test = ten1.reshape(1, 1, 4)
print(torch.matmul(test, ten2))
# 为了验证上面【相当于为了能够矩阵乘积，将ten1升维为(1, 1, 4)，相当于test = ten1.reshape(1, 1, 4)】
# 手动验证一遍，结果也成立

# 【第六种情况】torch.matmul涉及到3维及3维以上的运算，那么又分几种情况：如果第一个元素是三维批量矩阵，第二个是一维向量,也就是matrix X vector
# 本质和【第四种情况】一样，矩阵向量乘积
# 矩阵向量乘积结果的维度，为最高维度降一维
# 矩阵向量乘积的诀窍是，size为(5,7,8)的和size(8)相乘，得到size = (5,7)
ten1 = torch.rand(2, 3, 4)
ten2 = torch.rand(4)
print("ten1 =", ten1)
print("ten2 =", ten2)
print(torch.matmul(ten1, ten2))

'''
ten1 = tensor([[[0.3918, 0.4447, 0.5737, 0.3538],
         [0.6733, 0.1397, 0.2998, 0.9738],
         [0.4240, 0.4311, 0.2714, 0.8561]],

        [[0.3020, 0.7965, 0.9188, 0.4578],
         [0.5988, 0.8072, 0.6782, 0.6739],
         [0.5107, 0.3325, 0.2850, 0.5148]]])
ten2 = tensor([0.7973, 0.9619, 0.6213, 0.0514])
tensor([[1.1147, 0.9074, 0.9653],
        [1.6014, 1.7099, 0.9306]])
'''

