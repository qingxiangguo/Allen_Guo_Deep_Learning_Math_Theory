# _*_ coding=utf-8 _*_

'''
通过一个简单的神经网络示例来说明一维向量（input）和二维矩阵（other）的乘积运算。

在这个神经网络例子中，神经网络有三层：

输入层：输入层有 3 个神经元，对应输入向量 x 的 3 个特征。在计算过程中，输入向量 x 的形状为 (3,)。
隐藏层：隐藏层有 4 个神经元。输入层到隐藏层的权重矩阵 W 的形状为 (4, 3)/或者(3, 4)，表示有 4 个隐藏神经元，每个神经元有 3 个权重值。
计算输入层与隐藏层之间的连接后，我们得到隐藏层的激活值 hidden_activations（形状为 (4,)）。
输出层：输出层有 2 个神经元。隐藏层到输出层的权重矩阵 V 的形状为 (2, 4)/或者(4, 2)，表示有 2 个输出神经元，每个神经元有 4 个权重值。
计算隐藏层与输出层之间的连接后，我们得到输出层的激活值 output_activations（形状为 (2,)）。
总结一下，这个神经网络的结构是：输入层有 3 个神经元，隐藏层有 4 个神经元，输出层有 2 个神经元。

权重矩阵 W 的形状可以是 (4, 3) 也可以是 (3, 4)，这取决于我们如何表示权重矩阵和进行矩阵乘法。权重矩阵的本质就是表示输入层
和隐藏层之间神经元的连接权重。

使用形状为 (4, 3) 的权重矩阵 W 时，我们需要将输入向量 x 与转置后的权重矩阵 W.T 相乘。这样计算时，矩阵乘法遵循的规则是：
输入向量 x 的形状为 (3,)，转置后的权重矩阵 W.T 的形状为 (3, 4)，相乘后得到形状为 (4,) 的隐藏层激活值向量。

如果我们将权重矩阵表示为形状为 (3, 4) 的矩阵，那么我们可以直接将输入向量 x 与权重矩阵相乘，不需要进行转置操作。
此时矩阵乘法遵循的规则是：输入向量 x 的形状为 (3,)，权重矩阵的形状为 (3, 4)，相乘后得到形状为 (4,) 的隐藏层激活值向量。

这两种表示方式都是正确的，只要在计算过程中保持一致性即可。重要的是理解权重矩阵表示了输入层和隐藏层神经元之间的连接关系。

神经网络中，从a层，到b层，如果a有X个神经元，b有Y个神经元，那么b的权重矩阵就是(X, Y)的二维矩阵
'''

import torch

# 假设我们有一个输入向量 x，表示一组特征，形状为 (3,)：

x = torch.tensor([0.5, 0.6, 0.7])

# 我们有一个神经网络的权重矩阵 W，表示输入层到隐藏层的权重，形状为 (4, 3)：

W = torch.tensor([[0.1, 0.2, 0.3],
[0.4, 0.5, 0.6],
[0.7, 0.8, 0.9],
[1.0, 1.1, 1.2]])

# 在这个例子中，我们想要将输入向量 x 乘以权重矩阵 W 以得到隐藏层的激活值。根据之前的讨论，torch.matmul
# 可以处理一维向量和二维矩阵的乘法运算。因此，我们可以这样计算：

hidden_activations = torch.matmul(x, W.T) # 注意这里我们需要转置 W，使其形状变为 (3, 4)

# 计算结果如下：

hidden_activations = torch.tensor([0.3800, 0.9200, 1.4600, 2.0000])
# 这个就是一维向量(3)和二维矩阵(3,4)，矩阵乘法后的结果，为(1,4)，然后再删去预置维度

# 这里，我们得到一个形状为 (4,) 的一维向量 hidden_activations，表示隐藏层的激活值。
# 接下来，我们可以添加一个激活函数（例如 ReLU）来处理隐藏层的激活值，以引入非线性。这里我们使用 torch.relu() 函数来实现：

hidden_activations_relu = torch.relu(hidden_activations)

# 计算结果如下：

hidden_activations_relu = torch.tensor([0.3800, 0.9200, 1.4600, 2.0000])
# 隐藏层激活值都是正数，所以经过 ReLU 激活函数后的值保持不变

# 现在，我们需要将隐藏层的激活值传递给输出层。假设我们有一个权重矩阵 V，表示隐藏层到输出层的权重，形状为 (2, 4)：

V = torch.tensor([[0.05, 0.06, 0.07, 0.08],
[0.09, 0.10, 0.11, 0.12]])

# 我们可以使用 torch.matmul 将隐藏层的激活值与权重矩阵 V 进行矩阵乘法：

output_activations = torch.matmul(hidden_activations_relu, V.T) # 注意这里我们需要转置 V，使其形状变为 (4, 2)

# 计算结果如下：

output_activations = torch.tensor([0.7340, 1.6180])

# 最后，我们得到一个形状为 (2,) 的一维向量 output_activations，表示输出层的激活值。根据具体任务和需要，
# 可以在输出层添加适当的激活函数（例如 softmax、sigmoid 等）来得到最终预测结果。
