# _*_ coding=utf-8 _*_
# 张量乘法有四种操作，torch.mul,torch.mm, torch.bmm, torch.matmul
# 首先是torch.mul，支持广播
# 功能，对位相乘，可以处理两种情况，第一，input是矩阵/向量，other是标量，这个时候是就是input的所有元素乘上other
# 第二种情况，input是矩阵/向量，other是矩阵/向量，对位相乘
# torch.mul可以处理一维，二维，三维等多种情况，多了就不行了
import torch

a = torch.tensor([[1, 3],  # a是两行，两列
                 [2, 3]])

b = torch.tensor([[2, 4],
                 [1, 5]])

c = torch.mul(a, b) # 可以这种方式相乘
d = a * b # 重点！！！torch.mul也可以直接用*号表示，*是进行对位相乘，@是进行矩阵相乘，但是当*用于两个不是一模一样的矩阵，无法直接进行对位相乘，除非可以广播

e = a.mul(b)  # 也可以调用张量a的mul方法，将b传参进去
print(c)
print(d)
print(e) # 可以看到上面三个表示方式都是一样的

'''
tensor([[ 2, 12],
        [ 2, 15]])
tensor([[ 2, 12],
        [ 2, 15]])
tensor([[ 2, 12],
        [ 2, 15]])
'''

# 还要一种情况，是标量乘张量
c = 2 * a
print(c)
'''
tensor([[2, 6],
        [4, 6]])
'''
d = torch.mul(2, a)
print(d)
'''
tensor([[2, 6],
        [4, 6]])
'''
e = a.mul(2)
print(e)
'''
tensor([[2, 6],
        [4, 6]])  # 可以看到，上面三个表示方式都是一样的
'''

# 另外，torch.mul(a, b)是矩阵a和b对应位相乘
# 对两者的size有要求，如果形状不是一模一样，就要判断能否可以进行广播，那么会广播后，再进行对位相乘
# 下面的两个数组，shape分别为(2,2)与(3,2)，从右到做，虽然2相同，但是2与3不满足广播，所以无法处理

a = torch.tensor([[1, 3],
                 [2, 3]])

b = torch.tensor([[1, 8],
                [2, 5],
                [2, 3]])

# print(a * b) # 会报错

a = torch.tensor([[1.1207],  # 4行1列
        [-0.3137],
        [0.0700],
        [0.8378]])

b = torch.tensor([[0.5146,  0.1216, -0.5244,  2.2382]])  # 1行4列

print(a * b) # a的shape为（4，1），b的为(1，4)，从右到左，4，1和1，4满足广播条件，所以会扩展为一个4，4的矩阵
'''
tensor([[ 0.5767,  0.1363, -0.5877,  2.5084],
        [-0.1614, -0.0381,  0.1645, -0.7021],
        [ 0.0360,  0.0085, -0.0367,  0.1567],
        [ 0.4311,  0.1019, -0.4393,  1.8752]])
'''

a = torch.tensor([[ 1.1207],  # a是8行一列
                  [ 1.1207],
                  [-0.3137],
                  [ 1.1207],
                  [ 0.0700],
                  [ 1.1207],
                  [ 0.8378],
                  [ 1.1207]])

b = torch.tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])  # 一行4列

print('a * b结果是', a * b)  # a的shape为（8，1），b的为(1，4)，从右到左，4，1和1，8满足广播条件，所以会扩展为一个8，4的矩阵

'''
tensor([[ 0.5767,  0.1363, -0.5877,  2.5084],  
        [ 0.5767,  0.1363, -0.5877,  2.5084],
        [-0.1614, -0.0381,  0.1645, -0.7021],
        [ 0.5767,  0.1363, -0.5877,  2.5084],
        [ 0.0360,  0.0085, -0.0367,  0.1567],
        [ 0.5767,  0.1363, -0.5877,  2.5084],
        [ 0.4311,  0.1019, -0.4393,  1.8752],
        [ 0.5767,  0.1363, -0.5877,  2.5084]])
'''

print('torch,matmul结果是', torch.matmul(a, b))

'''
tensor([[ 0.5767,  0.1363, -0.5877,  2.5084],  # 变成四行八列
        [ 0.5767,  0.1363, -0.5877,  2.5084],
        [-0.1614, -0.0381,  0.1645, -0.7021],
        [ 0.5767,  0.1363, -0.5877,  2.5084],
        [ 0.0360,  0.0085, -0.0367,  0.1567],
        [ 0.5767,  0.1363, -0.5877,  2.5084],
        [ 0.4311,  0.1019, -0.4393,  1.8752],
        [ 0.5767,  0.1363, -0.5877,  2.5084]])
'''

print('torch,mul结果是', torch.mul(a, b))

# 以上结果证明，torch.mul也可以直接用*号表示，但是当*用于两个不是一模一样的矩阵，无法进行对位相乘
# 时，如果满足广播条件，也就是两个数组是可广播的，会自动填充到相同维度相点乘,*是严格执行点乘操作，指代torch.mul

# 再来一个例子

a = torch.ones(3, 4)
b = torch.tensor([1,2,3]).reshape(3, 1) # 变为二维句子，三行一列
print(a)
print(b)
print(torch.mul(a, b))  # 当a, b两个矩阵仅仅用*表示，但是无法点乘，也无法矩阵乘法时，会自动扩充到相同维度，然后完成点乘，这是广播机制

'''
分析： 第一个矩阵shape是(3, 4)，第二个是(3, 1)，形状不一样，但满足广播条件（有1存在，且3等于3），广播后，将第二个shape扩充为(3, 4)，然后两者进行对位相乘
tensor([[1., 1., 1., 1.],   # 3行4列
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
tensor([[1],   # 3行1列
        [2],
        [3]])
tensor([[1., 1., 1., 1.],
        [2., 2., 2., 2.],
        [3., 3., 3., 3.]])
        
会把第二个数组自动扩充为

tensor([[1, 1, 1, 1],   # 3行4列
        [2, 2, 2, 2],
        [3, 3, 3, 3]])

'''

a = torch.tensor([[1, 2, 3],   # 2行三列
                 [4, 5, 6]])

b = torch.tensor([[1, 2],   # 3行两列
                  [3, 4],
                  [5, 6]])

# a的shape为(2,3)，b的shape为(3,2),a和b是无法广播的（因为3不等于2），所以在torch.mul进行对位相乘时，无法通过广播来改变两个矩阵的shape，那么torch.mul自然也就无法运用了

# print(a * b) # a和b没法进行基于mul的元素对应相乘（最简单直观的），但是可以进行矩阵相乘

# 再来一个广播机制例子，矢量乘矩阵, vector X matrix
x = torch.tensor([1, 2, 3])

y = torch.tensor([[5,6,7],  # 3行3列
                  [8,9,10],
                  [2,3,4]])

print(torch.mul(x, y))
print(torch.mul(y, x))

# x的shape为(3)，y的shape为(3, 3)，两者shape不同，那么就看torch.mul能否通过广播机制（广播机制会改变维度），将两者变为相同了，否则无法计算
# 首先两者满足广播，因为3等于3，因此x的shape首先会变为(1, 3)，然后最后变为(3 ,3)（因为每个维度往最大的去取）
# 然后再与y对位相乘

'''
实际上就是把x扩展为了
torch.tensor([[1, 2, 3],
             [1, 2, 3],
             [1, 2, 3]])
'''

'''
tensor([[ 5, 12, 21],
        [ 8, 18, 30],
        [ 2,  6, 12]])
tensor([[ 5, 12, 21],
        [ 8, 18, 30],
        [ 2,  6, 12]])
'''
'''
tensor([[ 5, 12, 21],
        [ 8, 18, 30]])
'''

