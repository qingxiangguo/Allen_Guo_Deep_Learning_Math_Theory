# _*_ coding=utf-8 _*_

"""
一个简单的梯度下降示例，其中我们计算了损失函数对参数w的梯度，并将其用于更新w的值。这个过程是迭代的，每一次更新都会让损失函数的值逐渐减小，
直到达到最小值或者收敛到一个接近最小值的值。

假设我的神经网络只有，一个输入层，一个输出层，每个都只有一个节点，简单映射为y = w*x + 3，
需要优化的参数是w（其实就是斜率）。假设我现在只有一个样本，(3, 12)，也就是真实值为12。
现在我来演示梯度下降的过程。假设随机初始化，w = 1，那么y = 3 + 3 = 6，
损失函数 = (12 - 6)^2 = 36。那么我们的目的，就是要减少损失函数的值。
现在求损失函数对于参数的梯度，dL/dw = (12 - (x*w + 3))^2/dw = -2x*(9-w*x)，
将w = 1，x = 3代入得，dL/dw = -36。设学习率为0.1，
w优化为w = w - 0.05 * -36 = 1 + 3.6 = 4.6, 损失函数 = (12 -16.8 )^2 = 23.04，发现损失函数
确实变小了

我的例子是一个简单的回归问题，其中神经网络只有一个输入节点和一个输出节点。我需要通过训练神经网络来找到一个最优的参数w，
使得预测值y能够更接近真实值12。

在初始化w=1后，我计算了预测值y=6，并且计算了损失函数（预测值和真实值之间的误差）为36。然后我使用梯度下降算法来更新w的值，以使损失函数尽可能减小。
梯度下降算法是一种常用的优化算法，可以通过计算损失函数对于参数w的梯度来确定参数的更新方向和步长，进而更新w的值。

在这个例子中，计算出的梯度为-36，说明当前的w偏大，需要减小w才能让损失函数减小。我使用学习率为0.1的步长，将w更新为4.6，
此时预测值y=21.8，损失函数为23.04，比之前的损失函数36要小，说明参数更新是有效的。

接下来，我可以重复上述步骤，不断更新w的值，直到损失函数达到一个可接受的水平或者训练达到一定的迭代次数。这样就可以训练出一个简单的神经网络，用于对未知数据做出预测。

下一轮梯度下降步骤如下：

计算当前参数 w=4.6 对应的预测值 y = wx + 3 = 4.63 + 3 = 16.8。

计算当前损失函数的值，即 (y-12)^2 = (16.8-12)^2 = 23.04。

计算损失函数对参数 w 的梯度，dL/dw = -2x*(9-wx) = -2*3*(9-4.6*3) = 28.8。

根据梯度下降算法的更新规则，更新参数 w：w = w - 学习率 * 梯度 = 4.6 - 0.1 * (28.8) = 1.72。
可以发现w参数在向真实值靠近

重复步骤1-4，直到达到满意的损失函数值或迭代次数。
"""