# _*_ coding=utf-8 _*_

"""
使用一阶泰勒展开证明，梯度下降确实能让目标函数（损失函数）越来越小

推导出在当前点朝着梯度的反方向下降会让目标函数的值变小。这个结论也是梯度下降算法的基础，
因为梯度就是目标函数在当前点的导数，
它指向了函数值增长最快的方向，所以朝着梯度的反方向下降就能让函数值变小

假设w是你要优化的参数, w(n+1) = w(n) - α* L(w(n))'，这是第一个公式 （1） 其中L(w(n))'就是损失函数，对参数的导数，也就是梯度

L(w)是代价函数，我们的目的就是让他变小

把L(w) 看成f(x)，把w(n)看成x0，L(w(n))看成f(x0)

L(w)函数，在w(n)处的泰勒一阶展开式是，L(w) = L(w(n)) + L(w(n))'(w-w(n))  （2）

可以用这个公式求任意w的值，只要w不要离w(n)太远

由于学习率一般不是很大，我们默认w 距离 w(n+1)不是太远的

那么我们令w = w(n+1)，并带入公式2，也就是使用泰勒近似，求w(n+1)处的值

L(w(n+1)) = L(w(n)) + L(w(n))'(w(n+1)-w(n))

变形移项， L(w(n+1)) - L(w(n)) = L(w(n))'(w(n+1)-w(n))  (3)

将公式 (1) (w(n+1)-w(n)) = -α* L(w(n))'  代入公式（3）的右边

所以，L(w(n+1)) - L(wn) = -α* [L(w(n))']^2

又因为学习率肯定是个正数，所以可得，L(w(n+1)) - L(wn) < 0

由此证明了梯度下降算法
"""