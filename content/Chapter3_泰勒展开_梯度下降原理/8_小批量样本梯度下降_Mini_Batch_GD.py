# _*_ coding=utf-8 _*_

"""
小批量样本梯度下降(Mini-batch Gradient Descent) 指的是每次迭代使用若干个（
而不是全部）样本来计算损失函数和梯度的值, 并进行参数更新。这个batch的大小通常比较小，
通常取32, 64或者128. 这样可以在保证每次更新都有“全局”视野的同时，减少了每次迭代所需的计算量

假设我有三个点是(4,3), (1,3), (3, 7), 这三个点是训练用的三个样本。
用线y=3x+b拟合这三个点， 使用小批量梯度下降, 学习率=0.01, b初始值为0

具体实现上，我们会设置一个batch_size，表示每次随机抽取的样本数。在本例子中，
我们设置batch_size=2，则每次会随机抽取两个样本，在这两个样本上计算梯度并进行参数更新

【第一轮迭代】

选择(4,3), (1,3)

真实值 y1 = 3, 预测值 y^1 = 3*x1 + b = 3(4) + 0 = 12

损失函数L1 = (y真实值 - y预测值)^2 = (3 - 12)^2 = 81

真实值 y2 = 3, 预测值 y^2 = 3*x2 + b = 3(1) + 0 = 3

损失函数L2 = (y真实值 - y预测值)^2 = (3 - 3)^2 = 0

梯度Δ1 = -2(y真实值 - y预测值) = 18  （指的是L对b的梯度，也就是L对你要优化的参数的梯度）
梯度Δ2 = -2(y真实值 - y预测值) = 0

平均梯度 = (18 + 0 )/2 = 9 （所有梯度的平均值）

我们更新参数 b = b - η * 梯度 = 0 - η * (9) = -0.01 * (9) = -0.09

【第二轮迭代】

选择 (1,3), (3, 7)

真实值 y2 = 3, 预测值 y^2 = 3*x2 + b = 2.91

损失函数L2 = (y真实值 - y预测值)^2 = 0.0081

真实值 y3 = 7, 预测值 y^3 = 3*x3 + b = 8.91

损失函数L3 = (y真实值 - y预测值)^2 = 3.6481

梯度Δ2 = -2(y真实值 - y预测值) = -0.18
梯度Δ3 = -2(y真实值 - y预测值) = 3.82

平均梯度 = (-0.18 + 3.82)/2 = 1.82

"""