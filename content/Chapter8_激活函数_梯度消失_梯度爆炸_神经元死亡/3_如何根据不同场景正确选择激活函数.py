# _*_ coding=utf-8 _*_

"""
选择输出层激活函数的主要考虑因素是要解决的具体问题类型。以下是一些常见的问题类型和推荐的激活函数：

二分类问题：sigmoid函数通常是最佳选择。输出层的神经元应该只有一个，它将输出一个介于0和1之间的概率值，表示样本属于某一类的概率。

多分类问题：softmax函数通常是最佳选择。输出层的神经元数量应该与类别数量相同，并输出每个类别的概率。

回归问题：通常不需要输出层激活函数，因为输出层需要产生连续数值。

决定隐藏层的激活函数通常需要一些实验和调整，因为没有通用的规则可以适用于所有情况。但是，一般而言，ReLU是最流行的选择，因为它相对简单且计算速度快。
但是，如果使用ReLU导致模型无法收敛，可以尝试其他激活函数，例如Leaky ReLU、ELU或GELU。
如果需要避免梯度消失问题，可以使用ReLU、Leaky ReLU或ELU。
如果需要对输入的负值响应较强，可以使用Leaky ReLU、ELU或GELU。

神经网络的输入层节点不需要激活函数，在定义输入层时无需担心激活函数。
输出层激活函数取决于我们要解决的问题类型。在回归问题中，我们使用线性（恒等）激活函数。在二元分类器中，
我们使用 sigmoid 激活函数。在多类分类问题中，我们使用 softmax 激活函数。在多标签分类问题中，我们使用 sigmoid 激活函数，为每个类输出一个概率值。
隐藏层中使用非线性激活函数，通过考虑模型的性能或损失函数的收敛性来做出选择。可以从 ReLU 激活函数开始，如
果你有一个Dying ReLU 问题，试试leaky ReLU。
在 MLP 和 CNN 神经网络模型中，ReLU 是隐藏层的默认激活函数。
在 RNN 神经网络模型中，我们对隐藏层使用 sigmoid 或 tanh 函数。tanh 函数具有更好的性能。
只有恒等激活函数被认为是线性的。所有其他激活函数都是非线性的。
不要在隐藏层中使用 softmax 和恒等函数，在隐藏层中使用 tanh、ReLU、ReLU 的变体、swish 和 hard swish 函数。
hard swish是最新的研究结果，可以试试。

当我们训练神经网络时，激活函数被用来确定神经元输出的非线性程度。这对于神经网络的表达能力非常关键，因为如果没有激活函数，
神经网络将只能执行线性变换，无法处理非线性的数据。

下面是一些激活函数的例子，以及在不同场景下如何选择激活函数：

线性激活函数
线性激活函数最常用于回归问题，它的表达式是 f(x) = x。当我们使用线性激活函数时，神经元的输出直接与输入成正比，不会被压缩或拉伸。
这使得它非常适合处理连续的数值型数据。例如，在预测房价时，使用线性激活函数可以让模型直接预测价格而不需要做出任何修正。

Sigmoid 函数
Sigmoid 函数可以把一个任意实数转换成一个 0 到 1 之间的实数。这个函数在二元分类问题中非常有用，因为它的输出可以被解释为概率。
例如，我们可以使用 sigmoid 函数来预测某个人是否患有某种疾病。如果 sigmoid 函数的输出接近 0，我们就可以认为这个人没有这种疾病；
如果接近 1，我们就可以认为这个人患有这种疾病。

ReLU 函数
ReLU 函数是神经网络中最常用的激活函数之一。它的表达式是 f(x) = max(0, x)。当输入小于等于 0 时，ReLU 函数输出 0；
当输入大于 0 时，ReLU 函数输出输入值本身。ReLU 函数在处理图像、语音和文本等非线性数据时表现出色。
例如，在图像分类中，ReLU 函数可以帮助神经网络学习不同形状的物体和纹理。

Softmax 函数
Softmax 函数在多类分类问题中非常有用。它可以把一个向量转换成一个概率分布，其中每个元素的取值都在 0 到 1 之间，且所有元素的和等于 1。
例如，在图像分类中，我们可以使用 softmax 函数来预测图像属于哪个类别。Softmax 函数可以帮助我们计算每个类别的概率，并找出概率最大的那个类别。

在选择激活函数时，我们需要考虑问题的性质、数据的特点以及模型的性能。如果我们处理的是连续的数值型数据，我们可以选择线性激活函数；
如果我们处理的是二元分类问题，我们可以选择 sigmoid 函数；如果我们处理的是图像、语音、文本等非线性问题，则通常需要在隐藏层中使用非线性激活函数，
以便模型能够学习到更复杂的特征表示。其中，ReLU 是常用的激活函数之一，
由于其计算速度快，且在处理大规模图像数据时表现良好，因此被广泛应用于深度学习模型中。

另外，由于深度学习模型中的反向传播算法要求激活函数是可导的，因此我们需要避免使用不可导的激活函数。
例如，有些人可能会使用阶跃函数（step function）来作为激活函数，但是它在许多点上都是不可导的，因此不能用于深度学习模型中。

在选择激活函数时，我们通常会根据问题的性质、数据集的特点以及模型的架构来决定。
例如，在使用卷积神经网络（Convolutional Neural Networks，CNN）处理图像时，ReLU 函数往往是一个不错的选择，而在处理文本数据时，
可能需要使用其他激活函数，例如 tanh 或者 LSTM 中的 sigmoid 函数。

当我们选择激活函数时，我们需要考虑以下因素：

【非线性性】：神经网络的激活函数必须是非线性的，否则多个神经元之间就无法发挥作用。因为线性函数的组合仍然是线性函数，
所以如果我们使用线性激活函数，则神经网络只能实现线性变换。
【可微性】：我们通常使用反向传播算法来更新神经网络的参数，因此激活函数必须是可微的。如果激活函数不是可微的，则反向传播无法计算梯度。
【计算效率】：某些激活函数可能会在计算上比其他激活函数更有效，因此在设计神经网络时需要考虑计算效率。

基于这些因素，我们可以根据问题类型和神经网络的结构来选择合适的激活函数。例如：

对于二元分类问题，Sigmoid函数通常比其他激活函数更适合。因为Sigmoid函数的输出范围在0到1之间，可以表示概率，
可以将输出视为样本属于某个类别的概率。在输出层使用Sigmoid函数，可以将输出值限制在0到1之间，并可以将输出解释为概率。

对于多元分类问题，Softmax函数通常是最好的选择。Softmax函数可以将输出归一化为一个概率分布，
因此可以将输出视为每个类别的概率。使用Softmax函数可以确保输出层的所有神经元的输出总和为1。

对于隐藏层，ReLU函数是通用的激活函数，并且在大多数情况下都能很好地工作。ReLU函数不仅易于计算，
还可以避免梯度消失问题。但是，ReLU函数也有一个缺点，即一些神经元可能在训练过程中变得“死亡”，即永远无法激活。为了解决这个问题，
可以使用PReLU函数，它是ReLU函数的变体。PReLU函数引入了一个小的负斜率，可以避免神经元死亡。

当然，以上只是基于经验的选择，根据具体问题的特征进行调整和优化。
"""