# _*_ coding=utf-8 _*_

"""
(x_1, ..., x_n）是信号向量，它和权重（w_1, ..., w_n）相乘。然后再累加（即求和 + 偏置项 b）。
最后，激活函数 f 应用于累加的总和

权重（w_1, ..., w_n）和偏置项 b 对输入信号进行线性变换。而激活函数对该信号进行非线性变换，
这使得我们可以任意学习输入和输出之间的复杂变换

深度学习中的神经元模型通常由两个步骤组成：

线性变换：输入信号与权重向量相乘，再加上偏置项。
非线性变换：将线性变换的结果传递给激活函数，得到神经元的输出。
如果没有激活函数，那么整个神经网络就会变成一个线性模型，这会严重限制网络的表达能力，无法处理非线性问题。
因此，激活函数在深度学习中起到引入非线性因素的重要作用。

举个例子，如果我们要训练一个神经网络来识别手写数字，那么输入数据将是一个由像素值组成的向量。
如果没有激活函数，神经元将只能执行线性变换，无法识别数字中的曲线和轮廓等非线性特征。
但是，如果我们在神经元后添加一个非线性激活函数，例如sigmoid或ReLU，神经元就可以学习输入数据的非线性特征，
从而提高网络的表达能力和识别精度。
"""