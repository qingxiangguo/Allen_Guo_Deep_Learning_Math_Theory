# _*_ coding=utf-8 _*_

"""
寻找损失函数的最低点，就像我们在山谷里行走，希望找到山谷里最低的地方。那么如何寻找损失函数的最低点呢？
在这里，我们使用了微积分里导数，通过求出函数导数的值，从而找到函数下降的方向或者是最低点（极值点）。

损失函数里一般有两种参数，一种是控制输入信号量的权重(Weight, 简称w），
另一种是调整函数与真实值距离的偏差（Bias，简称b）。我们所要做的工作，就是通过梯度下降方法，
不断地调整权重w和偏差b，使得损失函数的值变得越来越小。

实际的项目里样本数据会有很多个，每一个样本都有自己的损失函数和梯度
对于每一个样本数据，我们都可以求出一个梯度（也就是各自损失函数对于权重的偏导）。这个时候，
我们需要把各个样本数据的权重梯度加起来，并求出它们的平均值，用这个平均值来作为样本整体的权重梯度。

现在知道了  需要前进的方向，接下来需要知道应该前进多少。这里我们用到学习率(Learning Rate)这个概念。
通过学习率，可以计算前进的距离（步长）。

我们用w0表示权重的初始值，w1表示更新后的权重值，用a表示学习率，则有：
w1 = w0 - a*（所有样本的平均梯度）

在梯度下降中，我们会重复式子多次，直至总体代价函数收敛不变。

所以我可以这么理解，梯度下降中，使用了每个样本的各自的损失函数，来求各自的梯度，最后求所有梯度的平均值
然后优化。整体代价函数并没有被用于优化，只是一个结果的评估，优化一次，看一下代价函数的大小

梯度下降优化算法通过对每个样本的损失函数求导并计算平均梯度，来进行参数更新，
而不是直接使用全局损失函数（代价函数）。同时，你也可以用整体损失函数来检验模型的效果

我们把上面的内容稍微整理一下，可以得到梯度下降的整体过程：

for i = 0 to 训练数据的个数：

(1) 计算第 i 个训练数据的权重w和偏差b相对于损失函数的梯度。于是我们最终会得到每一个训练数据的权重和偏差的梯度值。

(2) 计算所有训练数据权重w的梯度的总和。

(3) 计算所有训练数据偏差b的梯度的总和。

2. 做完上面的计算之后，我们开始执行下面的计算：

(1) 使用上面第(2)、(3)步所得到的结果，计算所有样本的权重和偏差的梯度的平均值。

(2) 使用下面的式子，更新每个样本的权重值和偏差值。

w1 = w0 - a*（所有样本的平均梯度）
b1 = b0 - a*（所有样本的平均梯度）

【注意】每次更新后，每个样本的损失函数都会变，每个样本的梯度也会变，总体的平均梯度也会变，优化的情况也会变

重复上面的过程，直至损失函数收敛不变。

"""