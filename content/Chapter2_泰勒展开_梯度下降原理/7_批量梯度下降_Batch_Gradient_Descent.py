# _*_ coding=utf-8 _*_

"""
批量梯度下降（Batch Gradient Descent）是最常用的梯度下降算法之一。
它的基本思想是在每次迭代时使用全部训练样本来计算梯度。这种方法在每次迭代中都使用所有样本来计算损失函数的梯度，
并用这个梯度来更新参数。由于使用所有样本计算梯度，所以叫做批量梯度下降。

优点是每次迭代都可以使用所有样本，因此可以确保收敛到全局最优解。缺点是当训练样本数非常大时，
每次迭代都需要使用大量的计算资源，会变得非常慢。

批量梯度下降算法通过计算所有样本的梯度来更新参数。在每一次迭代中，首先将所有样本输入到模型中，然后计算每个样本对应的损失函数。
接着，我们对于每个样本计算损失函数对于参数的偏导数(梯度)。
最后，我们将所有样本的梯度平均，并使用这个平均值来更新参数。
由于需要处理所有样本，所以算法运行时间较长，
但是更新参数时会使用所有样本信息，可能会得到更准确的结果。

下面使用例子：

假设我有三个点是(4,3), (1,3), (3, 7), 这三个点是训练用的三个样本。
用线y=3x+b拟合这三个点， 使用BGD梯度下降, 学习率=0.01, b初始值为0, 目的是找最合适的b点

【第一轮迭代】
真实值 y1 = 3, 预测值 y^1 = 3*x1 + b = 3(4) + 0 = 12

损失函数L1 = (y真实值 - y预测值)^2 = (3 - 12)^2 = 81

真实值 y2 = 3, 预测值 y^2 = 3*x2 + b = 3(1) + 0 = 3

损失函数L2 = (y真实值 - y预测值)^2 = (3 - 3)^2 = 0

真实值 y3 = 7, 预测值 y^3 = 3*x3 + b = 3(3) + 0 = 9

损失函数L3 = (y真实值 - y预测值)^2 = (7 - 9)^2 = 4

梯度Δ1 = -2(y真实值 - y预测值) = 18  （指的是L对b的梯度，也就是L对你要优化的参数的梯度）
梯度Δ2 = -2(y真实值 - y预测值) = 0
梯度Δ3 = -2(y真实值 - y预测值) = 4

平均梯度 = (18 + 0 + 4)/3 （所有梯度的平均值）

【用来评估收敛性能的】整体损失函数 J = 所有损失函数的平均值

= 1/3 * (L1 + L2 + L3) = 1/3 * (81 + 0 + 4) = 29

我们更新参数 b = b - η * 梯度 = 0 - η * (22/3) = -0.01 * (22/3) = -0.0733

【第二轮迭代】

(4,3), (1,3), (3, 7)

真实值 y1 = 3, 预测值 y^1 = 3*x1 + b  = 11.9267

损失函数L1 = (y真实值 - y预测值)^2 = 79.68597289

真实值 y2 = 3, 预测值 y^2 = 3*x2 + b = 2.9267

损失函数L2 = (y真实值 - y预测值)^2 = 0.00537289

真实值 y3 = 7, 预测值 y^3 = 3*x3 + b = 8.9267

损失函数L3 = (y真实值 - y预测值)^2 = 3.71217289

梯度Δ1 = -2(y真实值 - y预测值) = 17.8534
梯度Δ2 = -2(y真实值 - y预测值) = -0.1466
梯度Δ3 = -2(y真实值 - y预测值) = 3.8534

平均梯度 =  （所有梯度的平均值）= 7.18673333

【用来评估收敛性能的】整体损失函数 J = 所有损失函数的平均值

= 1/3 * (L1 + L2 + L3) = 27.80117289 (可见比29更小了)

我们更新参数 b = b - η * 梯度 = 0 - η * (7.18673333) = -0.0733 - 0.01* （7.18673333）

b = -0.14516733

依次类推
"""