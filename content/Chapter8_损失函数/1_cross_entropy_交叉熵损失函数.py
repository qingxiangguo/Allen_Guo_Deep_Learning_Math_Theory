# _*_ coding=utf-8 _*_

"""
交叉熵的概念见信息论基础

交叉熵损失函数（Cross-entropy loss function）是一种常用的损失函数，主要用于衡量分类任务中，模型预测的概率分布与真实概率分布之间的差异。它通常用于深度学习中的监督学习任务，如分类问题。

交叉熵损失函数的计算公式为：

H(P, Q) = -∑ P(x) * log(Q(x))

其中，P(x) 是真实概率分布，Q(x) 是模型预测的概率分布。对数通常以自然数 e 或者 2 为底。

在实际应用中，交叉熵损失函数通常与 softmax 函数结合使用，用于多分类任务。softmax 函数将模型的原始输出转换为概率分布，
使得所有类别的概率之和为 1。

交叉熵损失函数的目的是最小化模型预测与真实概率分布之间的差距，从而提高模型在分类任务中的准确性。

假设我们有一个多分类问题，类别数为3，分别是猫、狗和人。我们的神经网络模型需要对输入图片进行分类。

首先，模型会输出一个长度为3的概率分布向量y_pred，表示对每个类别的预测概率。例如，模型对一个输入图片的预测结果可能是：y_pred = [0.2, 0.5, 0.3]。这表示模型认为这个图片是猫的概率为0.2，是狗的概率为0.5，是人的概率为0.3。

接下来，我们需要将真实标签表示成一个长度为3的one-hot向量。假设这个图片真的是一张狗的图片，那么真实标签y_true应该为：y_true = [0, 1, 0]。

现在我们可以计算交叉熵损失函数的值。根据公式L(y_true, y_pred) = - Σ[y_true_i * log(y_pred_i)]，我们可以得到：

L(y_true, y_pred) = -(0 * log(0.2) + 1 * log(0.5) + 0 * log(0.3)) = -log(0.5) ≈ 0.301



"""