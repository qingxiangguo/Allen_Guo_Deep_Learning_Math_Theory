# _*_ coding=utf-8 _*_

"""
让我们尝试一个简化的具体例子。假设我们有一个只有两个输入神经元和一个输出神经元的单层神经网络。由于余弦损失函数需要两个输入向量，我们将使用这个简化的神经网络计算两个输入向量的余弦相似度。

我们的神经网络结构如下：

输入层 (2) -> 全连接层(1) -> 输出层 (1)

假设我们的输入向量 a 和 b 如下：

a = [1, 2]
b = [3, 4]

W = [[0.5], [0.6]]

注意，a和b是同一次训练中的一对输入，需要进行比较，具体过程为：

a和b分别输入给模型，各获得输出，然后对输出计算余弦相似度，然后根据余弦相似度计算余弦损失函数

a_output = [1 * 0.5 + 2 * 0.6] = [1.7]
b_output = [3 * 0.5 + 4 * 0.6] = [3.3]

此时，余弦相似度为：

cos_sim = (1.7 * 3.3) / (sqrt(1.7^2) * sqrt(3.3^2)) = 1 # 永远为1

我们假设 y = 1，即 a_input 和 b_input 是相似的。在这种情况下，损失函数是：

loss = 1 - cos_sim = 0

现在我们需要计算损失函数关于 w1 的梯度。为了计算这个梯度，我们需要使用链式法则：

d(loss) / d(w1) = d(loss) / d(cos_sim) * d(cos_sim) / d(a_output) * d(a_output) / d(w1)

根据损失函数，我们可以得到：

d(loss) / d(cos_sim) = -1

接下来，我们计算余弦相似度关于 a_output 的梯度：

cos_sim = 1 # 恒等于1， 所以余弦相似度关于 a_output 的梯度 = 0

最后，我们计算 a_output 关于 w1 的梯度：

d(a_output) / d(w1) = d(1 * w1 + 2 * w2) / d(w1) = 1

现在我们可以将这三个梯度相乘，得到损失函数关于 w1 的梯度：

d(loss) / d(w1) = -1 * 0 * 1 = 0

注意，这个结果是针对一个特定的例子。在实际训练中，梯度通常不会是 0，因为权重、输入和标签在训练过程中会不断更新和变化。
此外，这个例子中的网络和损失函数非常简单，复杂的网络和损失函数会导致更复杂的梯度计算。

在这个简化的例子中，我们确实只有一个输出神经元，因此计算出的余弦相似度确实总是为1。在实际应用中，输出层通常会包含多个神经元，
比如12_余弦损失函数实例中的每个样本有300个元素，输出也有300个元素，对应这里每个元素有2个元素，输出有1个元素
以便在多维空间中表示数据。这样可以更好地计算不同输入之间的相似度，并提高模型的性能。

在实际应用中，如果我们想使用余弦相似度来比较两个向量，那么网络的输出层应该包含多个神经元。这样可以确保输出是多维向量，可以在多维空间中表示数据。
这有助于更准确地计算不同输入之间的相似度，并提高模型的性能。

实际不需要你手算，算图和自动微分技术大大简化了神经网络中梯度计算的过程，尤其是在处理复杂的网络结构和损失函数时。

动态计算图（例如 PyTorch 使用的）在每次前向传播时构建新的计算图，这为动态网络结构提供了灵活性。它允许我们在运行时更改网络结构，
这对于处理可变长度的输入或者具有条件分支的网络非常有用。

计算图记录了张量之间的操作关系以及如何从输入到输出。在反向传播时，PyTorch 使用链式法则，从最后的损失值开始，向前计算每个参数的梯度。
这种自动微分技术使得我们不必手动计算复杂的梯度表达式，而是让 PyTorch 自动完成这些工作。这在求解复杂损失函数和网络结构时非常有用，
为我们节省了大量的时间和精力。
"""