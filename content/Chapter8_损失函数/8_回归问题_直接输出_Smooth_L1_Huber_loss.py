# _*_ coding=utf-8 _*_

"""
Smooth L1损失和Huber损失在某种程度上是相同的。实际上，Smooth L1损失是Huber损失的一种变种。它们都是为了解决平方损失（MSE）对
异常值敏感以及L1损失在0处不光滑的问题。两者都在损失函数中结合了L1和L2（平方）损失的特点。

MAE损失在极值点附近梯度非常大，在极值点处非常不稳定，但对异常点不敏感；MSE对异常点敏感，但在接近极值点时梯度逐渐减小至0，可以得到精确极值。

Huber Loss对于包含异常点的数据集一般表现由于以上两者，异常值以MAE方式处理，极值点以MSE方式处理。

假设我们有一个简单的神经网络，它需要预测一个连续值。在这个示例中，我们将使用一组输入数据点（x）和对应的目标值（y）。对于每个输入数据点，
神经网络将输出一个预测值（f(x)）。

现在，我们可以使用Huber损失或Smooth L1损失来计算每个数据点的损失。我们首先看一下Huber损失。

对于Huber损失，我们需要先确定一个阈值delta。损失函数的计算方式如下：

如果|y - f(x)| <= delta，则损失为0.5 * (y - f(x))^2。
如果|y - f(x)| > delta，则损失为delta * |y - f(x)| - 0.5 * delta^2。

接下来，我们看一下Smooth L1损失。Smooth L1损失实际上是Huber损失在delta = 1时的特例。损失函数的计算方式如下：

如果|y - f(x)| <= 1，则损失为0.5 * (y - f(x))^2。
如果|y - f(x)| > 1，则损失为|y - f(x)| - 0.5。

通过计算这两种损失函数，我们可以得到每个数据点的损失值。然后，我们可以通过求平均损失来计算整个数据集的总损失。在训练神经网络时
，我们将尝试最小化这个总损失，这通常通过梯度下降或其他优化算法来实现。

------------------------------

有两个样本ytrue = (0, 1), ypredict = (0.6, 0.4)，求Smooth L1损失

给定两个样本，我们可以按照Smooth L1损失的定义逐一计算每个样本的损失。Smooth L1损失的计算方式如下：

如果|y_true - y_pred| <= 1，则损失为 0.5 * (y_true - y_pred)^2。
如果|y_true - y_pred| > 1，则损失为 |y_true - y_pred| - 0.5。
现在，我们将逐一计算这两个样本的损失。

样本1：
y_true = 0
y_pred = 0.6
差值 = |y_true - y_pred| = |0 - 0.6| = 0.6

由于差值 <= 1，我们使用第一个公式：
损失1 = 0.5 * (y_true - y_pred)^2 = 0.5 * (0 - 0.6)^2 = 0.5 * 0.36 = 0.18

样本2：
y_true = 1
y_pred = 0.4
差值 = |y_true - y_pred| = |1 - 0.4| = 0.6

由于差值 <= 1，我们同样使用第一个公式：
损失2 = 0.5 * (y_true - y_pred)^2 = 0.5 * (1 - 0.4)^2 = 0.5 * 0.36 = 0.18

现在我们已经计算出两个样本的损失值，我们可以求平均损失来得到总损失：
总损失 = (损失1 + 损失2) / 2 = (0.18 + 0.18) / 2 = 0.36 / 2 = 0.18

"""