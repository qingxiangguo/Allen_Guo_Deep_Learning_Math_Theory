# _*_ coding=utf-8 _*_

"""
二分类任务中输出层常用sigmoid激活函数，并且使用二分类交叉熵损失函数

现在我们来看一个猫狗二分类问题。我们有一个神经网络，用于识别图片中的动物是猫还是狗。在这个问题中，我们将猫标记为 1，狗标记为 0。

神经网络的输出层使用 sigmoid 激活函数，将原始输出转换为介于 0 和 1 之间的概率值。假设网络对一个输入图片的输出为：

sigmoid_output = 0.7

这意味着模型预测这个图片是猫的概率为 0.7，是狗的概率为 1 - 0.7 = 0.3。

假设这个输入图片实际上是一只猫（标签 y = 1）。我们可以使用二分类交叉熵损失函数来衡量模型预测与真实标签之间的差距。计算公式为：

Binary_Cross_Entropy_Loss = -(y * log(sigmoid_output) + (1 - y) * log(1 - sigmoid_output))

对于这个例子，二分类交叉熵损失为：

(1 * log(0.7) + (1 - 1) * log(1 - 0.7))
这个损失值等于 -log(0.7)。在训练过程中，模型将不断调整参数以最小化这个损失值。当模型训练得越好，二分类交叉熵损失值会越小，
这意味着模型对输入图片的预测准确性越高。

现在，让我们考虑一个相反的情况，假设输入图片实际上是一只狗（标签 y = 0）。对于同样的模型输出（sigmoid_output = 0.7），
我们可以使用二分类交叉熵损失函数来计算损失值：

Binary_Cross_Entropy_Loss = -(y * log(sigmoid_output) + (1 - y) * log(1 - sigmoid_output))

在这个例子中，二分类交叉熵损失为：

(0 * log(0.7) + (1 - 0) * log(1 - 0.7))
这个损失值等于 -log(1 - 0.7) = -log(0.3)。

总之，二分类交叉熵损失函数可以有效地衡量模型在二分类任务中的表现。通过最小化这个损失函数，模型将不断调整参数以提高预测准确性。
在实际应用中，可以根据实际问题选择合适的激活函数和损失函数来优化模型表

"""