# _*_ coding=utf-8 _*_

"""
多分类任务中输出层常用softmax激活函数，并且使用交叉熵损失函数

下面所有log默认以2为底

我们以一个简化的数字识别问题为例，假设我们有一个神经网络，用于识别手写数字 0、1 和 2。我们使用 one-hot 编码表示每个数字的标签，即：

数字 0 对应 [1, 0, 0]
数字 1 对应 [0, 1, 0]
数字 2 对应 [0, 0, 1]
现在，假设我们有一个手写数字 2 的输入样本。对应的真实标签为 one-hot 向量 [0, 0, 1]。

神经网络经过训练后，输出层使用 softmax 激活函数，它将原始输出转换为概率分布。假设网络对这个输入样本的输出概率分布为：

[0.1, 0.3, 0.6]

这表示模型预测该样本为数字 0、1 和 2 的概率分别为 0.1、0.3 和 0.6。

为了衡量模型预测与真实标签之间的差距，我们可以使用交叉熵损失函数。计算公式为：

H(P, Q) = -∑ P(x) * log(Q(x))

对于这个例子，交叉熵损失为：

-[(0 * log(0.1)) + (0 * log(0.3)) + (1 * log(0.6))]

注意，我们只关心数字 2（真实标签）对应的部分，因为其他部分的真实概率 P(x) 为 0，它们在求和中不会产生贡献。

所以交叉熵损失为：

log(0.6)
计算出这个损失值后，我们希望通过训练模型来最小化它。在训练过程中，模型将不断调整参数，以使得预测概率分布更接近真实概率分布，
从而降低交叉熵损失。当模型训练得越好，交叉熵损失会越小，模型对输入样本的预测准确性也会越高。
"""